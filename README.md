Documentation is available at [hydra-mq.marcoapp.io](https://hydra-mq.marcoapp.io/).

<div style="text-align: center; background-color: #00B4D8; padding: 32px; border-radius: 32px; margin-bottom: 16px;">
  <svg preserveAspectRatio="xMidYMid meet" viewBox="96.07 56.7 512.93 501.15" xmlns="http://www.w3.org/2000/svg" style="max-height: 200px;">
    <g transform="matrix(.1 0 0 -.1 0 604)">
      <path d="m2976 5408c-105-189-119-342-45-475 16-29 26-53 22-53-5 0-39 9-77 20s-72 18-74 15c-9-9 19-90 55-155 19-36 90-144 157-240 68-96 141-204 164-240 69-111 132-251 149-335l17-80-32-44c-39-53-42-98-9-128 22-20 111-80 178-121l35-21 95 60c103 67 149 105 149 126 0 20-29 74-53 100-20 22-20 25-5 102 28 141 102 277 306 564 131 186 198 296 217 361 8 27 13 51 11 53-2 3-38-5-78-16-90-26-95-26-75-3 56 62 80 202 53 315-17 75-71 198-103 236l-20 24-6-61c-12-115-66-228-130-274l-29-21-66 65c-101 98-258 238-267 238-4 0-81-68-171-150l-163-151-26 17c-68 45-120 165-127 297-1 20-5 37-10 37-4 0-23-28-42-62zm232-778c43-19 59-34 92-83 22-32 40-63 40-68 0-27-108 15-134 52-9 13-16 34-16 48 0 15-13 36-32 54-36 31-31 31 50-3zm665 1c-21-17-33-37-33-52 0-14-7-35-16-48-26-36-134-79-134-53 0 21 79 128 101 138 114 52 128 55 82 15z" />
      <path d="m2513 4591c-65-61-165-128-285-191-93-48-198-94-204-88-2 2 19 50 46 106 45 94 48 103 27 96-100-30-238-151-332-294-85-126-184-336-285-600-99-257-110-279-151-315-26-21-39-25-63-21-47 10-98 50-151 121-27 36-50 65-51 65s-26-70-54-156c-49-145-52-162-48-229 14-216 209-352 532-371l101-6-63 38c-35 21-87 60-114 87l-50 48 188 106c104 57 290 164 414 236s248 139 275 149c106 37 179 17 217-60 18-39 20-56 15-155-8-176-75-395-209-681-118-255-146-368-127-516 13-105 34-171 86-273 113-219 331-436 642-642 125-82 388-231 396-224 2 3-33 45-77 94-302 337-463 623-512 904-43 250-3 454 168 855 134 316 168 437 169 606 1 123-21 210-78 305-101 168-302 294-559 350l-29 6 31 32c57 60 109 193 151 390 26 120 50 277 42 277-3-1-29-23-58-49zm-626-803c-18-13-27-29-27-47 0-14-7-37-16-50-19-26-87-60-122-61h-23l27 51c31 55 85 109 112 109 9 1 28 7 42 15 35 19 41 7 7-17z" />
      <path d="m4470 4633c0-5 9-64 20-133 41-251 103-445 166-518l35-41-28-6c-83-17-231-72-304-113-117-65-225-174-272-273-45-96-60-164-60-269 1-158 38-292 160-578 154-362 193-505 193-710 0-144-19-253-67-382-82-222-260-491-472-712-40-43-71-78-67-78 12 0 179 87 261 135 549 325 855 689 873 1040 7 142-13 222-105 425-93 202-153 357-188 475-93 322-62 495 89 495 71-1 127-26 386-177 124-72 304-175 400-228 96-54 177-99 179-102 10-8-109-110-165-141l-59-33 100 5c296 16 495 137 534 324 6 29 11 67 11 85 0 31-100 347-110 347-3 0-16-19-30-41-34-55-123-136-159-144-74-16-109 35-220 321-169 435-273 626-423 774-61 61-147 117-207 135-22 7-20-1 33-99 31-58 52-106 46-106-17 0-185 78-263 122-92 52-169 107-228 163-47 45-59 52-59 38zm806-889c24-29 65-106 60-112-2-2-21 0-41 4-53 10-105 58-105 97 0 22-9 38-32 58l-33 28 65-24c36-14 74-37 86-51z" />
      <path d="m3035 4165c-49-13-124-36-165-51-82-29-305-138-314-153-3-5 6-12 20-15 41-11 163-73 215-111l48-34 98 39c54 21 149 51 211 66 71 18 112 32 112 41 0 21-48 141-77 194-16 27-35 49-43 49-8-1-55-12-105-25z" />
      <path d="m3855 4146c-24-41-89-206-83-210 2-1 59-16 128-33 69-16 165-46 214-65l87-36 84 53c45 28 110 62 144 75 34 12 61 26 61 30 0 12-237 125-333 159-93 32-234 71-261 71-8 0-27-20-41-44z" />
      <path d="m3330 3538c0-244-52-549-190-1118-122-504-146-659-137-875 4-88 12-154 27-205 59-205 190-362 405-485l78-44 51 26c155 78 306 220 373 349 74 142 88 210 88 424-1 211-15 312-91 627-154 638-193 831-219 1078-8 77-15 177-15 223 0 45-3 82-6 82-4 0-41-31-83-68-42-38-81-72-88-76s-46 23-97 69c-46 41-87 75-90 75s-6-37-6-82z" />
      <path d="m2030 3182c-91-54-189-111-218-127l-52-28-34-106c-164-507-101-1080 169-1536 251-424 673-745 1140-865 195-50 315-63 535-57 203 5 292 17 459 63 665 180 1194 741 1340 1419 34 159 44 276 38 458-7 214-41 404-104 571l-18 50-220 128c-121 70-223 128-227 128s12-30 36-67c103-160 181-352 223-549 23-113 27-152 27-314 0-137-4-211-18-285-21-116-79-300-128-406-84-181-241-390-389-518-564-489-1343-542-1959-134-138 92-332 280-429 418-121 173-219 404-260 617-76 396 8 833 224 1162 25 38 45 71 45 72 0 11-29-5-180-94z" />
    </g>
  </svg>
</div>

A high performance Postgres message queue implementation for NodeJs/TypeScript.

<br />

## Features

  - Zero dependencies.
  - Scales to thousands of queues with no performance penalty.
  - Distributes well across multiple processes/servers.
  - Messages can be enqueued as part of existing database transactions.
  - Scheduled/repeating messages.
  - Fine-grained per-queue settings for concurrency and capacity.
  - Flexible message prioritization settings.
  - Configurable message retry settings.
  - DB client agnostic.
  - High throughput: > 10,000 jobs per second.

<br />

## Quick Look

```typescript
import { Deployment } from 'hydramq'
import { Pool } from 'pg'

const pool = new Pool({
  connectionString: process.env.DATABASE_URL
})

const deployment = new Deployment({
  schema: 'hydra'
})

// Enqueue messages
const queue = deployment.queue('myQueue')

for (let i = 0; i < 500; i += 1) {
  await queue.message.enqueue({ payload: `Ping: ${i}`, databaseClient: pool })
}

const processFn = async (msg: string) => console.log(msg)

deployment.daemon.processor({ processFn, databaseClient: pool })
deployment.daemon.orchestrator({ databaseClient: pool })
```

<br />

## Setup & Installation

HydraMQ can be installed from npm via:

```bash
npm install hydra-mq
```

Once the package is installed, we need to install the requisite DB machinery. HydraMQ aims to be agnostic to the DB client/migration procedure and thus provides a simple `string[]` of well-formatted SQL commands to run as part of a migration to facilitate said installation.

```typescript
import { Deployment } from 'hydramq'

// Choose which postgres schema in which you wish to install hydraMQ.
const deployment = new Deployment({ schema: 'hydra' })

// Run these SQL commands as part of a DB migration.
const sqlCommands: string[] = deployment.installation()
```

N.B. the set of SQL commands generated is **not** idempotent and thus it is strongly recommended that they are executed within a transaction.

<br />

## Basic Usage

The most simple usage pattern for HydraMQ would be a single queue being processed by a processor daemon. Using your `deployment` (defined above), we first must instantiate a queue (what we name our queue is irrelevant in this particular usecase):

```typescript
const queue = deployment.queue('myQueue')
```

We can now enqueue messages to our queue by passing both a string payload and our database client (we will use `pg.Pool` for examples going forward - however, we will see below that it is trivial to use any other database client):

```typescript
await queue.message.enqueue({ payload: 'Hi!', databaseClient: pool })
```

Messages will remain enqueued until they are processed by a processor daemon. We can create one by passing in a function to process the messages and our database client:

```typescript
const processFn = async (msg: string) => console.log(msg)
const processor = deployment.daemon.processor({ processFn: processFn, databaseClient: pool })
```

It is important to spawn _at least_ one orchestrator daemon. The orchestrator is responsible for maintaining general system health as well as providing support for more advance features mentioned below (scheduling and retries):

```typescript
const orchestrator = deployment.daemon.orchestrator({ databaseClient: pool })
```

<br />

## Processor Concurrency

Processor daemons will poll for messages - processing them when available. If no messages are available it will "timeout" for some period of time before polling again.

By default, the processor daemon will process one message at a time. However, if we set the `executionConcurrency` of the processor, it will dispatch dequeued messages to a fleet of executors that can process messages concurrent with one another:

```typescript
const processor = deployment.daemon.processor({ 
  processFn: processFn, 
  databaseClient: pool,
  executionConcurrency: 10,
})
```

This setting is useful when dealing with long running, IO-bound jobs. However, we will run into throughput limitations with short-lived jobs as even though there are multiple executors, the processor will still dequeue messages sequentially. In this scenario, it is recommended to deploy _multiple_ processors - with each processor dequeuing messages concurrently affording a much greater message throughput:

```typescript
const processors = [
  deployment.daemon.processor({ 
    processFn: processFn, 
    databaseClient: pool,
    executionConcurrency: 10,
  }),
  deployment.daemon.processor({ 
    processFn: processFn, 
    databaseClient: pool,
    executionConcurrency: 10,
  }),
  deployment.daemon.processor({ 
    processFn: processFn, 
    databaseClient: pool,
    executionConcurrency: 10,
  }),
]
```

Finally, it is worth noting that HydraMQ daemons all run on a single thread. This is no problem for IO-bound work, however anything CPU intensive will cause significant performance issues. HydraMQ processors must be spread across multiple processes/servers to leverage additional CPU cores to mitigate this issue.

Unlike other queuing solutions, there is no need to ensure the orchestrator daemon remains a singleton. Multiple orchestrators spanning multiple processes will behave well with one another â€“ thus simply naively replicating your HydraMQ worker process is perfectly sensible vs. having to separate the orchestrator into its own dedicated singleton process.

<br />

## Multiple Queues

As mentioned in the features section, HydraMQ scales to thousands of queues (maximum throughput actually _increases_ as jobs are distributed across more queues) with no performance penalty. 

By default, processor daemons will process messages across *all* queues. However, we can specify a `queuePrefix` override when creating a processor daemon. This will restrict the processor to only working on matching queues.

```typescript
const queue = deployment.queue('myQueue')
const highPriorityQueue = deployment.queue('highPriorityQueue')

const processors = [
  deployment.processor({
    processFn: processFn,
    databaseClient: pool,
  }),
  deployment.processor({
    processFn: processFn,
    queuePrefix: 'highPriorityQueue',
    databaseClient: pool,
  }),
]
```

In the example above, we show a pattern by which certain workloads can be prioritized. We define normal and high priority queues along with two processors. Messages from both queues are serviced by the first processor, but the second _only_ services messages from the `highPriorityQueue` â€“ providing extra compute to messages in said queue.

N.B. queue names must be dot-separated (i.e. `foo.bar.baz`). Prefix matching will only work at dot boundaries and not within the individual name segments (e.g. `foo.bar` will match with `foo.bar.baz`, but `foo.ba` will not).

<br />

## Queue Configuration

Queues can be configured to constrain maximum concurrency. Queues with a specified maximum concurrency will only allow a certain number of messages to be processed simultaneously across all processors (N.B. this concurrency limit is truly global and thus will work across distinct daemons/processes/servers). Queues can also be configured to constrain their total capacity. Queues with a specified maximum capacity will prevent further messages from being enqueued should their limit be reached. 

We can set and clear queue configuration via:

```typescript
// N.B. a null value means the concurrency/capacity is be unrestricted.
await queue.config.set({
  maxConcurrency: 5,
  databaseClient: pool,
  maxCapacity: null,
})

await queue.config.clear({
  databaseClient: pool,
})
```

N.B. changes to concurrency don't propagate instantly and messages currently being procesed will certainly never be "evicted" in response to concurrency changes.

<br />

## Prioritized Messages

Messages can be prioritized - ensuring they are processed faster than others by setting the `priority` argument when enqueuing a message. All messages with specified priorities are processed _before_ those without any specific priority:

```typescript
await queue.message.enqueue({
  message: 'hello world',
  databaseClient: pool,
  priority: 10,
})
```

<br />

## Retryable Messages

Messages can be given a retry policy to ensure that should processing fail, messages are re-attempted at a later date. We simply provide the `numAttempts` and `timeoutSecs` arguments when enqueueing a message. `numAttempts` specifies the number of times a message can be processed before being archived in a failed state. `timeoutSecs` specifies the amount of time a message is "locked" (and unavailable for attempted re-processing) after a failure.

```typescript
await queue.message.enqueue({
  payload: 'hello world',
  databaseClient: pool,
  numAttempts: 5,
  timeoutSecs: 60 * 5,
})
```

<br />

## Scheduled Messages

Each queue can also have scheduled messages that enqueue repeatedly at fixed intervals. Scheduled messages can be set and cleared using the same arguments as we'd expect to see when enqueuing a message. An additional required argument is `repeatSecs` which describes how often the message should be scheduled:

```typescript
await queue
  .schedule('myScheduledMessage')
  .set({
    payload: 'hello world',
    databaseClient: pool,
    numAttempts: 5,
    priority: 10,
    timeoutSecs: 60 * 5,
    repeatSecs: 10,
  })

await queue
  .schedule('myScheduledMessage')
  .clear({
    databaseClient: pool,
  })
```

<br />

## Graceful Shutdown & Cleaning

Daemons (processors and orchestrators) can be gracefully shutdown by awaiting their `stop()` method. This ensures daemons finish any tasks they are currently working on before exiting.

Failure to gracefully shut down daemons (particularly processors) may result in messages being stuck in an invalid "processing" state. In this state they will occupy a concurrency slot inside their queue - potentially causing blockages and reducing job throughput.

That being said, hard crashes are a fact of life and ultimately can't be avoided. As such, we must be able to recover from theser situations. To that end, the orchestrator regularly performs a "clean" operation that sweeps these "stuck" jobs. It does this by considering all jobs that have been processing for longer than a specified `staleSecs` (set when a message is enqueued) as "stuck".

N.B. Make sure to set `staleSecs` such that it is larger than the expected amount of time required to process a job as otherwise the orchestrator is liable to "clean" it even if not stuck. By default this is set to 1 hour.

<br />

## Other Database Clients

Although `pg.Pool` has been used in the examples above, it is trivial for you to switch this for a database client of your choice by implementing the minimal HydraMQ `DatabaseClient` interface (`pg.Pool` and `pg.Client` already implement this interface). 

HydraMQ never uses explicit transactions and as such single connections as well as connection pools work perfectly fine as HydraMQ database clients:

```typescript
type MyDatabaseClientResult = {
  rows: Array<Record<string, unknown>>
}

export class MyDatabaseClient implements DatabaseClient {
  async query(sqlQuery: string): Promise<MyDatabaseClientResult> {
    // Implement here...
  }
}
```

<br />

## Events 

HydraMQ daemons emit the following events:

| Event Type | Description |
| ----------|-------------|
| `MESSAGE_DEQUEUED` | A message has been dequeued for processing. |
| `MESSAGE_PROCESSED` | A message has been successfully processed and was removed from the queue. |
| `MESSAGE_EXPIRED` | A message has failed to process and was removed from the queue. |
| `MESSAGE_LOCKED` | A message has failed to process - but will eventually be re-processed. |
| `MESSAGE_SCHEDULED` | A scheduled message has been enqueued. |
| `MESSAGE_UNLOCKED` | A previously locked message has been unlocked for re-processing. |
| `MESSAGE_CLEANED` | A message stuck in the "running" state has been cleaned. |

These events can be subscribed to by adding event handlers:

```typescript
deployment.daemon.addEventHandler(event => {
  console.log(event.eventType)
})
```

<br />

## Message Lifecycle

When a message is enqueued, it will start in the `READY` state if the queue has sufficient open concurrency slots. If this is not the case, messages will begin in a `WAITING` state.

Processors will grab messages in the `READY` state for processing. They do this in order of priority and then creation time. Once a message has been "dequeued" by a processor it transitions into the `PROCESSING` state. 

If processing completes successfully or fails with no retries remaining, the message is deleted. The queue from which the message originated will then be explicitly "advanced" - this involves potentially bringing a message in the `WAITING` state into the `READY` state, should the concurrency constraints of the queue allow (again in order of priority and creation time).

If processing fails with retries remaining, instead of being deleted, it will be transitioned into a `LOCKED` state. As above, the queue is again "advanced", potentially pulling another message into the `READY` state should it be allowed. 

Similar to how message enqueuing works, the orchestrator will transition locked messages into either the `WAITING` or `READY` state when they are ready for attempted re-processing.

Messages that have been stuck in the `PROCESSING` state for too long, will either be deleted or transitioned into a `LOCKED` state by the orchestrator for re-processing.

<br />

## Polling & Responsiveness

HydraMQ uses polling as its central mechanism for transitioning and processing messages. 

As a result, responsiveness can sometimes suffer with enqueued jobs not _immediately_ being picked up for processing due to processors being in a "timeout" period after previously finding no messages to process. 

We also see this lack of responsiveness with jobs not being unlocked immediately after their timeout period expires (for similar reasons).
